{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \\[3\\]신경망의 이해\n",
        "\n",
        "전준한  \n",
        "2023-12-23\n",
        "\n",
        "# `-` 퍼셉트론\n",
        "\n",
        "-   신경망의 기본단위\n",
        "\n",
        "`-` In deeplearning - \\$ y = ax +\n",
        "$<span style=\"color:red; font-weight:bold;\">$b\\$</span>\n",
        "\n",
        "=\\> $y =$ <span style=\"color:red;\">$w$</span>x +\n",
        "<span style=\"color:red;\">$b$</span>\n",
        "\n",
        "<br> 가중치 = $w$ <br> 바이어스 = $bias$ <br> 가중합 = $wx$ <br>\n",
        "활성화함수 = $a$\n",
        "\n",
        "\\# 다층 퍼셉트론의 설계\n",
        "\n",
        "\\$ n_1= (x_1w\\_{*{11}} + x_2w*{\\_{21}} + b_1) \\$\n",
        "\n",
        "-   $n_1$ 은 하나의 은닉층내의퍼셉트론의 값입니다.\n",
        "\n",
        "\\$ n_2= (x_1w\\_{*{12}} + x_2w*{\\_{22}} + b_1) \\$\n",
        "\n",
        "\\$y\\_{out} =(n_1w\\_{*{31}} + x_2w*{\\_{32}} + b_3) \\$"
      ],
      "id": "327e30d4-ce86-4223-bc52-f0ccf78244a1"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# W(1) 은 2x2 행렬로 볼수있다."
      ],
      "id": "97b4d1a3-3cbb-4c7f-a520-3b91513d03b1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "W_1 = \\[\\[w_11 ,w_12\\] ,\\[w_21,w_22\\]\\] <br> W_2 = \\[\\[w_31\\],\\[w_32\\]\\]\n",
        "<br> b(1) = \\[\\[3\\],\\[1\\]\\] <br> b(2) = \\[-1\\]"
      ],
      "id": "dff6cdba-937c-4a8b-89f4-30bf5daa216c"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# 역전파는 생략 -> 그냥 전미분으로 반대쪽으로 가면서 연쇄법칙으로 계산 용이"
      ],
      "id": "c36bc398-800d-4bcf-83e6-0cca697f202c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## `-` 기울기 소실 문제\n",
        "\n",
        "<br> \\#### 역전파가 전달될때마다 기울기의 값이 점점 작아져서 기울기 소실\n",
        "발생(vanishing gradient)\n",
        "\n",
        "-   이유 : sigmoid 의 미분 값이 0.3이 max\n",
        "-   시그모이드, 하이퍼볼릭,렐루,소프트멕스가 이쓴데 렐룰를 쓰면 미분값이\n",
        "    1이되어 맨 처음층이 살아남을 수 있어서 사용됩니다.\n",
        "\n",
        "`-` 다른 경사하강법문제 -확률적 경사 하강법 SGD , 모멘텀 , 네스테로프\n",
        "모멘텀 , 아다그라드 , RMSProp, 아담 등은 다음에 다루겠습니다."
      ],
      "id": "02075ff4-8ec7-4dc9-a273-3972255059c7"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "hf",
      "display_name": "Python (hf)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  }
}