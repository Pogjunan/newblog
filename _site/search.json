[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/[4]딥러닝기초실습.html",
    "href": "posts/[4]딥러닝기초실습.html",
    "title": "[4]딥러닝기초실습",
    "section": "",
    "text": "#모델 직접 설계 (tensorflow)\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\nimport numpy as np\nimport tensorflow as tf\n\ntf.random.set_seed(3)\n\n# data_set\nData_set = np.load(txt(\"....\",deliiter=\",\"))\n\n# X = Data_set[:,0:17]\n# Y = Data_set[:,17]\nmodel = Sequential()\nmodel.add(Dense(30,inpt_dim=17 , activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))"
  },
  {
    "objectID": "posts/[4]딥러닝기초실습.html#add-로-새로운-층을-만들고-dense-함수로-노드의-개수를-30개로-만듭니다.",
    "href": "posts/[4]딥러닝기초실습.html#add-로-새로운-층을-만들고-dense-함수로-노드의-개수를-30개로-만듭니다.",
    "title": "[4]딥러닝기초실습",
    "section": "add 로 새로운 층을 만들고 Dense 함수로 노드의 개수를 30개로 만듭니다.",
    "text": "add 로 새로운 층을 만들고 Dense 함수로 노드의 개수를 30개로 만듭니다.\n- input_dim 으로 변수개수를 만들고 dense 는 은닉층 + 입력층의 역할을 겸합니다. - 현재 30,17 은 17개수의 변수 -&gt; 30개의 변수로 늘리겠다는것입니다.\n\nmodel.compile(loss='mean_squared_error',optimizer='adam',metrics=['aaccuracy'])\nmodel.fit(X,Y,epochs=100,batch__size=10)\n\nmetric 함수는 모델이 컴파일될 때 모델 수행 결과 나타나게끔 설정하는 부분입니다."
  },
  {
    "objectID": "posts/[4]딥러닝기초실습.html#교차엔트로피",
    "href": "posts/[4]딥러닝기초실습.html#교차엔트로피",
    "title": "[4]딥러닝기초실습",
    "section": "교차엔트로피",
    "text": "교차엔트로피\n\n분류문제에서 많이 사용되며 이항인 경우 binary cross entropy 사용\n\n\n## 이후 쉬운  딥러닝 예시 나왔습니다."
  },
  {
    "objectID": "posts/[8] GAN1.html",
    "href": "posts/[8] GAN1.html",
    "title": "[8]GAN",
    "section": "",
    "text": "- CNN 과 RNN 후에 많은 연습할 알고리즘이 있지만 이 책에서는 CRNN 만 다루고 있고 이미 다뤄본 알고리즘이라 바로 GAN 만 다루고 끝내겠습니다.  - 무엇보다 가볍게 다루기 때문에 가볍게 보고 지나치겠습니다.\n\n\n\n현재 사진을 합성하는 기술로 존재하지 않는 사람을 그럴 듯 하게 만들 수 있는 기술\nGAN = 생성적 적대 신경망 ( Generative Adversarial Networks)\n매번 듣는 것 : 경찰과 도둑"
  },
  {
    "objectID": "posts/[8] GAN1.html#gan-오토인코더",
    "href": "posts/[8] GAN1.html#gan-오토인코더",
    "title": "[8]GAN",
    "section": "",
    "text": "현재 사진을 합성하는 기술로 존재하지 않는 사람을 그럴 듯 하게 만들 수 있는 기술\nGAN = 생성적 적대 신경망 ( Generative Adversarial Networks)\n매번 듣는 것 : 경찰과 도둑"
  },
  {
    "objectID": "posts/[8] GAN1.html#section",
    "href": "posts/[8] GAN1.html#section",
    "title": "[8]GAN",
    "section": "3.",
    "text": "3.\ngenerator.add(UpSampling2D()) # ...(1)\ngenerator.add(Conv2D(64,kernel_size=5,padding='same'))\n 5x5 커널을 사용하였고 padding 을 자동으로 조절하여 크기에 변화가 없도록 처리하였습니다.\n이 후 3x3 커널도 같은 작업으로 크기를 조절하였습니다.\n\n\n\ngenerator.add(Dense(128*7*7 , input_dim=100 , activation=LeakyReLU(0.2))) \ngenerator.add(Activation(LeakyReLU(0.2)))\n\nLeakyReLU 의 형태를 보면 \n\n로 x 값이 음수일 때 무조건 0 이 되어 뉴런이 일찍 소실되는 단점을 보완하였습니다. 0.2 는 0보다 작을경우 0.2 를 곱하라는 의미입니다."
  },
  {
    "objectID": "posts/[9] 마무리 GAN 실행과해석.html",
    "href": "posts/[9] 마무리 GAN 실행과해석.html",
    "title": "[9]GAN 실행과 마무리정리 (필독)",
    "section": "",
    "text": "생성자 : G() &gt; 판별자 : D() &gt; &gt; \n\n\n\ninput(그림형태)  =&gt;  생성자 : D(G(input))  =&gt;  생성된 그림 : G(input)  =&gt;  판별자 : 진짜? : D(G(input))=0 or 가짜? : D(x)=1 —-\n만들어야하는 것 :  D(G(input) 과 실제 D(x) 를 판별자가 더는 구별못하게 정확도 = 0.5 에 가까워지면 생성자의 역할이 종료\n코드 : 현재 코드는 GAN 중 페이스북의 AI 연구팀이 만든 DCGAN 입니다"
  },
  {
    "objectID": "posts/[9] 마무리 GAN 실행과해석.html#진행형식",
    "href": "posts/[9] 마무리 GAN 실행과해석.html#진행형식",
    "title": "[9]GAN 실행과 마무리정리 (필독)",
    "section": "",
    "text": "input(그림형태)  =&gt;  생성자 : D(G(input))  =&gt;  생성된 그림 : G(input)  =&gt;  판별자 : 진짜? : D(G(input))=0 or 가짜? : D(x)=1 —-\n만들어야하는 것 :  D(G(input) 과 실제 D(x) 를 판별자가 더는 구별못하게 정확도 = 0.5 에 가까워지면 생성자의 역할이 종료\n코드 : 현재 코드는 GAN 중 페이스북의 AI 연구팀이 만든 DCGAN 입니다"
  },
  {
    "objectID": "posts/[2]경사하강법과 다중선형회귀.html",
    "href": "posts/[2]경사하강법과 다중선형회귀.html",
    "title": "[2]경사하강법과 다중선형회귀",
    "section": "",
    "text": "import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndata = [[2,81],[4,93],[6,91],[8,97]]\nx=[i[0] for i in data]\ny=[i[1] for i in data]\n\nplt.figure(figsize=(8,5))\nplt.scatter(x,y)\nplt.show()\n\nx_data = np.array(x)\ny_data = np.array(y)\n\na=0\nb=0\n\nlr=0.03\nepochs=2001\n\nfor i in range(epochs):\n    y_pred = a* x_data + b\n    error = y_data - y_pred\n    a_diff = -(2/len(x_data)) * sum(x_data * (error))\n    b_diff = -(2/len(x_data)) * sum(error)\n    \n    a = a - lr * a_diff\n    b = b - lr * b_diff\n    if i % 100 == 0:\n        print(\"epoch=%.f , 기울기 =%.04f , 절편 = %.04f\" % (i,a,b))\n        \ny_pred = a * x_data + b\nplt.scatter(x,y)\nplt.plot([min(x_data),max(x_data)],[min(y_pred),max(y_pred)])\nplt.show()\n\n\n\n\n\n\n\n\nepoch=0 , 기울기 =27.8400 , 절편 = 5.4300\nepoch=100 , 기울기 =7.0739 , 절편 = 50.5117\nepoch=200 , 기울기 =4.0960 , 절편 = 68.2822\nepoch=300 , 기울기 =2.9757 , 절편 = 74.9678\nepoch=400 , 기울기 =2.5542 , 절편 = 77.4830\nepoch=500 , 기울기 =2.3956 , 절편 = 78.4293\nepoch=600 , 기울기 =2.3360 , 절편 = 78.7853\nepoch=700 , 기울기 =2.3135 , 절편 = 78.9192\nepoch=800 , 기울기 =2.3051 , 절편 = 78.9696\nepoch=900 , 기울기 =2.3019 , 절편 = 78.9886\nepoch=1000 , 기울기 =2.3007 , 절편 = 78.9957\nepoch=1100 , 기울기 =2.3003 , 절편 = 78.9984\nepoch=1200 , 기울기 =2.3001 , 절편 = 78.9994\nepoch=1300 , 기울기 =2.3000 , 절편 = 78.9998\nepoch=1400 , 기울기 =2.3000 , 절편 = 78.9999\nepoch=1500 , 기울기 =2.3000 , 절편 = 79.0000\nepoch=1600 , 기울기 =2.3000 , 절편 = 79.0000\nepoch=1700 , 기울기 =2.3000 , 절편 = 79.0000\nepoch=1800 , 기울기 =2.3000 , 절편 = 79.0000\nepoch=1900 , 기울기 =2.3000 , 절편 = 79.0000\nepoch=2000 , 기울기 =2.3000 , 절편 = 79.0000"
  },
  {
    "objectID": "posts/[2]경사하강법과 다중선형회귀.html#y-a_1x_1-a_2x_2-b",
    "href": "posts/[2]경사하강법과 다중선형회귀.html#y-a_1x_1-a_2x_2-b",
    "title": "[2]경사하강법과 다중선형회귀",
    "section": "- $ y = a_1x_1 + a_2x_2 + b $",
    "text": "- $ y = a_1x_1 + a_2x_2 + b $\n\n두 개의 독립변수 x_1 과 x_2 가 생긴 것으으로 a_1 과 a_2 구하는 것이 목표이다\n\n\n## 코딩으로 확인하기\n\n\ndata =[[2,0,81],[4,4,93],[6,2,91],[8,3,97]]\nx1 = [i[0] for i in data]\nx2 = [i[1] for i in data]\ny = [i[2] for i in data]\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits import mplot3d\n\nax = plt.axes(projection='3d')\nax.set_xlabel('study_hours')\nax.set_ylabel('private_class')\nax.set_zlabel('Score')\nax.scatter(x1,x2,y)\nplt.show()\n\n\n\n\n\n\n\n\n\n# 다중회귀의 경사하강법\n\nx1_data = np.array(x1)\nx2_data = np.array(x2)\ny_data = np.array(y)\n\na1 = 0\na2 = 0\nb = 0\n\n\nlr = 0.02\nepochs = 5000\nfor i in range(epochs):\n    y_pred = a1*x1_data + a2*x2_data + b\n    error = y_data - y_pred\n    a1_diff = -(2/len(x1_data)) * sum(x1_data *(error))\n    a2_diff = -(2/len(x2_data)) * sum(x2_data *(error))\n    b_diff = -(2/len(x1_data)) * sum(y_data - y_pred)\n    a1 = a1 - lr * a1_diff\n    a2 = a2 - lr * a2_diff\n    b = b - lr * b_diff\n\n\n    if i %100 == 0:\n        print(\"epoch=%.f , 기울기 = %.04f , 기울기a2 = %.04f , 절편 = %.04f\" % (i,a1,a2,b))\n        \n\nepoch=0 , 기울기 = 18.5600 , 기울기a2 = 8.4500 , 절편 = 3.6200\nepoch=100 , 기울기 = 7.2994 , 기울기a2 = 4.2867 , 절편 = 38.0427\nepoch=200 , 기울기 = 4.5683 , 기울기a2 = 3.3451 , 절편 = 56.7901\nepoch=300 , 기울기 = 3.1235 , 기울기a2 = 2.8463 , 절편 = 66.7100\nepoch=400 , 기울기 = 2.3591 , 기울기a2 = 2.5823 , 절편 = 71.9589\nepoch=500 , 기울기 = 1.9546 , 기울기a2 = 2.4427 , 절편 = 74.7362\nepoch=600 , 기울기 = 1.7405 , 기울기a2 = 2.3688 , 절편 = 76.2058\nepoch=700 , 기울기 = 1.6273 , 기울기a2 = 2.3297 , 절편 = 76.9833\nepoch=800 , 기울기 = 1.5673 , 기울기a2 = 2.3090 , 절편 = 77.3948\nepoch=900 , 기울기 = 1.5356 , 기울기a2 = 2.2980 , 절편 = 77.6125\nepoch=1000 , 기울기 = 1.5189 , 기울기a2 = 2.2922 , 절편 = 77.7277\nepoch=1100 , 기울기 = 1.5100 , 기울기a2 = 2.2892 , 절편 = 77.7886\nepoch=1200 , 기울기 = 1.5053 , 기울기a2 = 2.2875 , 절편 = 77.8209\nepoch=1300 , 기울기 = 1.5028 , 기울기a2 = 2.2867 , 절편 = 77.8380\nepoch=1400 , 기울기 = 1.5015 , 기울기a2 = 2.2862 , 절편 = 77.8470\nepoch=1500 , 기울기 = 1.5008 , 기울기a2 = 2.2860 , 절편 = 77.8518\nepoch=1600 , 기울기 = 1.5004 , 기울기a2 = 2.2859 , 절편 = 77.8543\nepoch=1700 , 기울기 = 1.5002 , 기울기a2 = 2.2858 , 절편 = 77.8556\nepoch=1800 , 기울기 = 1.5001 , 기울기a2 = 2.2858 , 절편 = 77.8563\nepoch=1900 , 기울기 = 1.5001 , 기울기a2 = 2.2857 , 절편 = 77.8567\nepoch=2000 , 기울기 = 1.5000 , 기울기a2 = 2.2857 , 절편 = 77.8569\nepoch=2100 , 기울기 = 1.5000 , 기울기a2 = 2.2857 , 절편 = 77.8570\nepoch=2200 , 기울기 = 1.5000 , 기울기a2 = 2.2857 , 절편 = 77.8571\nepoch=2300 , 기울기 = 1.5000 , 기울기a2 = 2.2857 , 절편 = 77.8571\nepoch=2400 , 기울기 = 1.5000 , 기울기a2 = 2.2857 , 절편 = 77.8571\nepoch=2500 , 기울기 = 1.5000 , 기울기a2 = 2.2857 , 절편 = 77.8571\nepoch=2600 , 기울기 = 1.5000 , 기울기a2 = 2.2857 , 절편 = 77.8571\nepoch=2700 , 기울기 = 1.5000 , 기울기a2 = 2.2857 , 절편 = 77.8571\nepoch=2800 , 기울기 = 1.5000 , 기울기a2 = 2.2857 , 절편 = 77.8571\nepoch=2900 , 기울기 = 1.5000 , 기울기a2 = 2.2857 , 절편 = 77.8571\nepoch=3000 , 기울기 = 1.5000 , 기울기a2 = 2.2857 , 절편 = 77.8571\nepoch=3100 , 기울기 = 1.5000 , 기울기a2 = 2.2857 , 절편 = 77.8571\nepoch=3200 , 기울기 = 1.5000 , 기울기a2 = 2.2857 , 절편 = 77.8571\nepoch=3300 , 기울기 = 1.5000 , 기울기a2 = 2.2857 , 절편 = 77.8571\nepoch=3400 , 기울기 = 1.5000 , 기울기a2 = 2.2857 , 절편 = 77.8571\nepoch=3500 , 기울기 = 1.5000 , 기울기a2 = 2.2857 , 절편 = 77.8571\nepoch=3600 , 기울기 = 1.5000 , 기울기a2 = 2.2857 , 절편 = 77.8571\nepoch=3700 , 기울기 = 1.5000 , 기울기a2 = 2.2857 , 절편 = 77.8571\nepoch=3800 , 기울기 = 1.5000 , 기울기a2 = 2.2857 , 절편 = 77.8571\nepoch=3900 , 기울기 = 1.5000 , 기울기a2 = 2.2857 , 절편 = 77.8571\nepoch=4000 , 기울기 = 1.5000 , 기울기a2 = 2.2857 , 절편 = 77.8571\nepoch=4100 , 기울기 = 1.5000 , 기울기a2 = 2.2857 , 절편 = 77.8571\nepoch=4200 , 기울기 = 1.5000 , 기울기a2 = 2.2857 , 절편 = 77.8571\nepoch=4300 , 기울기 = 1.5000 , 기울기a2 = 2.2857 , 절편 = 77.8571\nepoch=4400 , 기울기 = 1.5000 , 기울기a2 = 2.2857 , 절편 = 77.8571\nepoch=4500 , 기울기 = 1.5000 , 기울기a2 = 2.2857 , 절편 = 77.8571\nepoch=4600 , 기울기 = 1.5000 , 기울기a2 = 2.2857 , 절편 = 77.8571\nepoch=4700 , 기울기 = 1.5000 , 기울기a2 = 2.2857 , 절편 = 77.8571\nepoch=4800 , 기울기 = 1.5000 , 기울기a2 = 2.2857 , 절편 = 77.8571\nepoch=4900 , 기울기 = 1.5000 , 기울기a2 = 2.2857 , 절편 = 77.8571"
  },
  {
    "objectID": "posts/[6]자연어처리연습.html",
    "href": "posts/[6]자연어처리연습.html",
    "title": "[6]자연어처리연습",
    "section": "",
    "text": "자연어 처리 연습\n\n[순서]\n\n토큰화 \n원핫인코딩(보통 많이사용) \n단어임베딩 # 중요\n각자 필요한 EDA 시작\n\n\n\n가벼운 영화 리뷰 긍정,부정적 예측\n\nImport\n\n\nimport numpy \nimport tensorflow as tf\nfrom numpy import array\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Flatten,Embedding\n\n\ndocs = ['너무 재밌네요','최고예요','참 잘 만든 영화예요','추천하고 싶은 영화입니다.','한번 더 보고싶네요',' 글쎄요', \\\n        '별로예요','생각보다 지루하네요','연기가 어색해요.','재미없어요']\n# 긍정=1, 부정=0\nclasses = array([1,1,1,1,1,0,0,0,0,0])\n\n#토큰화\ntoken=Tokenizer()\ntoken.fit_on_texts(docs)\nprint(token.word_index)\n\n{'너무': 1, '재밌네요': 2, '최고예요': 3, '참': 4, '잘': 5, '만든': 6, '영화예요': 7, '추천하고': 8, '싶은': 9, '영화입니다': 10, '한번': 11, '더': 12, '보고싶네요': 13, '글쎄요': 14, '별로예요': 15, '생각보다': 16, '지루하네요': 17, '연기가': 18, '어색해요': 19, '재미없어요': 20}\n\n\n\n#패딩 , 서로 다른 길이를 4로 맞추기\nx = token.texts_to_sequences(docs) #keras 제공의 토큰만으로 채워진 인덱스 배열\nprint(x)\npadded_x = pad_sequences(x,4)\nprint(padded_x)\n\n[[1, 2], [3], [4, 5, 6, 7], [8, 9, 10], [11, 12, 13], [14], [15], [16, 17], [18, 19], [20]]\n[[ 0  0  1  2]\n [ 0  0  0  3]\n [ 4  5  6  7]\n [ 0  8  9 10]\n [ 0 11 12 13]\n [ 0  0  0 14]\n [ 0  0  0 15]\n [ 0  0 16 17]\n [ 0  0 18 19]\n [ 0  0  0 20]]\n\n\n\nx\n\n[[1, 2],\n [3],\n [4, 5, 6, 7],\n [8, 9, 10],\n [11, 12, 13],\n [14],\n [15],\n [16, 17],\n [18, 19],\n [20]]\n\n\n\n# 임베딩 입력 단어 수 지정\nword_size = len(token.word_index)+1\n\nfrom keras.utils import to_categorical\n\n#x = to_categorical(x,num_classes=word_size)\n#print(x) =&gt; 맨 앞에 0 추가되어 단어 수 보다 1 이 더 많게 인덱스 숫자 잡아줌.\n\n\n#단어 임베딩 포함한 딥러닝 모델 만들기 - tensorflow\nmodel = Sequential()\nmodel.add(Embedding(word_size,8,input_length=4)) #단어임베딩\nmodel.add(Flatten())\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(optimizer='adam', loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.fit(padded_x,classes,epochs=20)\n\nprint(\"\\n Accuracy: %.4f\" % (model.evaluate(padded_x,classes)[1]))\n\nEpoch 1/20\n1/1 [==============================] - 0s 325ms/step - loss: 0.6873 - accuracy: 0.7000\nEpoch 2/20\n1/1 [==============================] - 0s 3ms/step - loss: 0.6852 - accuracy: 0.7000\nEpoch 3/20\n1/1 [==============================] - 0s 2ms/step - loss: 0.6830 - accuracy: 0.7000\nEpoch 4/20\n1/1 [==============================] - 0s 3ms/step - loss: 0.6808 - accuracy: 0.8000\nEpoch 5/20\n1/1 [==============================] - 0s 3ms/step - loss: 0.6786 - accuracy: 0.8000\nEpoch 6/20\n1/1 [==============================] - 0s 3ms/step - loss: 0.6765 - accuracy: 0.8000\nEpoch 7/20\n1/1 [==============================] - 0s 3ms/step - loss: 0.6743 - accuracy: 0.8000\nEpoch 8/20\n1/1 [==============================] - 0s 2ms/step - loss: 0.6721 - accuracy: 0.8000\nEpoch 9/20\n1/1 [==============================] - 0s 3ms/step - loss: 0.6699 - accuracy: 0.9000\nEpoch 10/20\n1/1 [==============================] - 0s 2ms/step - loss: 0.6677 - accuracy: 0.9000\nEpoch 11/20\n1/1 [==============================] - 0s 3ms/step - loss: 0.6655 - accuracy: 1.0000\nEpoch 12/20\n1/1 [==============================] - 0s 3ms/step - loss: 0.6633 - accuracy: 1.0000\nEpoch 13/20\n1/1 [==============================] - 0s 3ms/step - loss: 0.6611 - accuracy: 1.0000\nEpoch 14/20\n1/1 [==============================] - 0s 3ms/step - loss: 0.6589 - accuracy: 1.0000\nEpoch 15/20\n1/1 [==============================] - 0s 3ms/step - loss: 0.6567 - accuracy: 1.0000\nEpoch 16/20\n1/1 [==============================] - 0s 3ms/step - loss: 0.6545 - accuracy: 1.0000\nEpoch 17/20\n1/1 [==============================] - 0s 2ms/step - loss: 0.6522 - accuracy: 1.0000\nEpoch 18/20\n1/1 [==============================] - 0s 3ms/step - loss: 0.6499 - accuracy: 1.0000\nEpoch 19/20\n1/1 [==============================] - 0s 3ms/step - loss: 0.6477 - accuracy: 1.0000\nEpoch 20/20\n1/1 [==============================] - 0s 3ms/step - loss: 0.6454 - accuracy: 1.0000\n1/1 [==============================] - 0s 80ms/step - loss: 0.6431 - accuracy: 1.0000\n\n Accuracy: 1.0000\n\n\n[해석] - 단어 임베딩 : 16차원의 벡터를 4차원의 벡터로 바꾼 것. - 단어간의 유사도를 통해 밀집된 정보로 공간 정보를 줄임. - 오차역전파를 이용하여 최적의 유사도 계산과정 거침. - keras 의 Embedding 함수 이용 (입력과 출력 크기를 결정) - 패딩 : 길이를 똑같이 맞춰주는 작업 - [3] 과 [4,5,6,7] 은 길이가 달라서 학습이 안되어 맞추어주어야한다. - keras 의 pad_sequence() 함수로 원하는 길이보다 짧은 부분은 숫자 0 을 채우고 길면 잘라서 길이 맞춥니다.\n\nword_size = len(token.word_index) +1\n\nword_size 는 인덱스가 몇 개 입력되어야하는지를 정합니다.\n1번째 단어 -&gt; 0 번 , 2번째 단어 -&gt; 1번 … 따라서 +1 로 길이를 (n-1)+1 = n 으로 맞출 수 있습니다."
  },
  {
    "objectID": "posts/[7] RNN.html",
    "href": "posts/[7] RNN.html",
    "title": "[7]RNN",
    "section": "",
    "text": "이전의 입력의 변수를 계속 사용한다는 것에 의미가 존재 ( 시퀀스에 유리)\nRNN 은 잦은 개선으로 LSTM 의 등장\n\n\n\n\nRNN 내부의 구조도 개선한 방식\n기초 RNN 구조의 기울기 소실 문제를 보완한 방법으로 반복 직전의 다음 층으로 기억된 값을 넘길지 안 넘길지 관리하는 단계를 추가한 방법\n\n\n\n\n\n\n1 Import\n\nfrom keras.datasets import reuters\nfrom keras.models import Sequential\nfrom keras.layers import Dense,LSTM,Embedding\nfrom keras.preprocessing import sequence\n\n\nfrom tensorflow.keras.utils import to_categorical\n\n\nimport numpy\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\n\nseed = 0\nnumpy.random.seed(seed)\ntf.random.set_seed(3)\n\n[2] 모델 만들기\n\n(X_train,Y_train),(X_test,Y_test) = reuters.load_data(num_words=1000, test_split=0.2)\ncategory = numpy.max(Y_train) +1\nprint(category,'카테고리')\nprint(len(X_train),'학습용뉴스')\nprint(len(X_test),'테스트용뉴스')\nprint(X_train[0])\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n2110848/2110848 [==============================] - 0s 0us/step\n46 카테고리\n8982 학습용뉴스\n2246 테스트용뉴스\n[1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 2, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 2, 2, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 2, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n\n\n\n# x_train =sequence.pad_sequences(X_train,maxlen=100)\n# x_test =sequence.pad_sequences(X_test,maxlen=100)\n# y_train = np_utils.to_categorical(Y_train) #이전코드\n# y_test = np_utils.to_categorical(Y_test)\n\n\nfrom tensorflow.keras.utils import to_categorical\n\nx_train = sequence.pad_sequences(X_train, maxlen=100)\nx_test = sequence.pad_sequences(X_test, maxlen=100)\ny_train = to_categorical(Y_train)  # np_utils.to_categorical 대신 to_categorical 사용\ny_test = to_categorical(Y_test)\n\n\nmodel = Sequential()\nmodel.add(Embedding(1000,100))\nmodel.add(LSTM(100,activation='tanh'))\nmodel.add(Dense(46,activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\nhistory = model.fit(x_train,y_train,batch_size=100,epochs=20,validation_data=(x_test,y_test))\n\nEpoch 1/20\n90/90 [==============================] - 9s 82ms/step - loss: 2.5827 - accuracy: 0.3460 - val_loss: 2.2901 - val_accuracy: 0.3620\nEpoch 2/20\n90/90 [==============================] - 7s 78ms/step - loss: 2.3067 - accuracy: 0.3917 - val_loss: 2.0693 - val_accuracy: 0.5036\nEpoch 3/20\n90/90 [==============================] - 7s 77ms/step - loss: 2.0138 - accuracy: 0.4958 - val_loss: 1.8937 - val_accuracy: 0.5285\nEpoch 4/20\n90/90 [==============================] - 7s 74ms/step - loss: 1.7928 - accuracy: 0.5442 - val_loss: 1.7842 - val_accuracy: 0.5677\nEpoch 5/20\n90/90 [==============================] - 7s 74ms/step - loss: 1.6979 - accuracy: 0.5669 - val_loss: 1.7077 - val_accuracy: 0.5628\nEpoch 6/20\n90/90 [==============================] - 7s 75ms/step - loss: 1.6262 - accuracy: 0.5826 - val_loss: 1.6480 - val_accuracy: 0.5761\nEpoch 7/20\n90/90 [==============================] - 7s 73ms/step - loss: 1.5504 - accuracy: 0.6097 - val_loss: 1.5901 - val_accuracy: 0.6024\nEpoch 8/20\n90/90 [==============================] - 7s 75ms/step - loss: 1.4472 - accuracy: 0.6346 - val_loss: 1.4807 - val_accuracy: 0.6238\nEpoch 9/20\n90/90 [==============================] - 7s 74ms/step - loss: 1.3671 - accuracy: 0.6525 - val_loss: 1.4441 - val_accuracy: 0.6296\nEpoch 10/20\n90/90 [==============================] - 7s 73ms/step - loss: 1.2649 - accuracy: 0.6828 - val_loss: 1.3604 - val_accuracy: 0.6625\nEpoch 11/20\n90/90 [==============================] - 7s 74ms/step - loss: 1.1974 - accuracy: 0.6939 - val_loss: 1.3254 - val_accuracy: 0.6652\nEpoch 12/20\n90/90 [==============================] - 7s 74ms/step - loss: 1.1243 - accuracy: 0.7096 - val_loss: 1.2726 - val_accuracy: 0.6754\nEpoch 13/20\n90/90 [==============================] - 7s 74ms/step - loss: 1.0576 - accuracy: 0.7306 - val_loss: 1.2636 - val_accuracy: 0.6759\nEpoch 14/20\n90/90 [==============================] - 7s 74ms/step - loss: 1.0125 - accuracy: 0.7390 - val_loss: 1.2701 - val_accuracy: 0.6768\nEpoch 15/20\n90/90 [==============================] - 7s 73ms/step - loss: 0.9626 - accuracy: 0.7554 - val_loss: 1.2027 - val_accuracy: 0.6946\nEpoch 16/20\n90/90 [==============================] - 7s 73ms/step - loss: 0.9166 - accuracy: 0.7675 - val_loss: 1.2308 - val_accuracy: 0.6874\nEpoch 17/20\n90/90 [==============================] - 7s 73ms/step - loss: 0.8661 - accuracy: 0.7816 - val_loss: 1.1993 - val_accuracy: 0.6968\nEpoch 18/20\n90/90 [==============================] - 7s 73ms/step - loss: 0.8367 - accuracy: 0.7877 - val_loss: 1.1866 - val_accuracy: 0.7057\nEpoch 19/20\n90/90 [==============================] - 7s 74ms/step - loss: 0.7863 - accuracy: 0.8044 - val_loss: 1.1989 - val_accuracy: 0.7030\nEpoch 20/20\n90/90 [==============================] - 7s 73ms/step - loss: 0.7522 - accuracy: 0.8096 - val_loss: 1.2209 - val_accuracy: 0.7075\n\n\n\nprint(\"\\n Test Accuracy: %.4f\" % (model.evaluate(x_test,y_test)[1]))\n\ny_vloss = history.history['val_loss']\ny_loss = history.history['loss']\n\nx_len = numpy.arange(len(y_loss))\nplt.plot(x_len,y_vloss,marker='.' , c=\"red\",label='Tesset_loss')\n\nplt.plot(x_len,y_loss,marker='.' , c=\"blue\" , label='Trainset_loss')\n\nplt.legend(loc='upper right')\nplt.grid()\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()\n\n71/71 [==============================] - 1s 11ms/step - loss: 1.2209 - accuracy: 0.7075\n\n Test Accuracy: 0.7075\n\n\n\n\n\n\n\n\n\n\n# 테스트오차가 올라가기직전에 끊어서 earlystopping 을 수동적으로 잘 했다고 볼 수 있다!\n\n\nEmbedding(1000,100) 은 [6] 에서 언급했 듯 1000차원을 100차원으로 축소시킨것\n100차원에서 LSTM 을 이용한 RNN 으로 오차역전파를 사용하며 각각의 노드를 학습\nDense 는 fullyconnected 를 의미하며 풀리커넥티드 46 node 로 layer 층을 softmax 시키기"
  },
  {
    "objectID": "posts/[7] RNN.html#lstm-long-short-term-memory",
    "href": "posts/[7] RNN.html#lstm-long-short-term-memory",
    "title": "[7]RNN",
    "section": "",
    "text": "RNN 내부의 구조도 개선한 방식\n기초 RNN 구조의 기울기 소실 문제를 보완한 방법으로 반복 직전의 다음 층으로 기억된 값을 넘길지 안 넘길지 관리하는 단계를 추가한 방법\n\n\n\n\n\n\n1 Import\n\nfrom keras.datasets import reuters\nfrom keras.models import Sequential\nfrom keras.layers import Dense,LSTM,Embedding\nfrom keras.preprocessing import sequence\n\n\nfrom tensorflow.keras.utils import to_categorical\n\n\nimport numpy\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\n\nseed = 0\nnumpy.random.seed(seed)\ntf.random.set_seed(3)\n\n[2] 모델 만들기\n\n(X_train,Y_train),(X_test,Y_test) = reuters.load_data(num_words=1000, test_split=0.2)\ncategory = numpy.max(Y_train) +1\nprint(category,'카테고리')\nprint(len(X_train),'학습용뉴스')\nprint(len(X_test),'테스트용뉴스')\nprint(X_train[0])\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n2110848/2110848 [==============================] - 0s 0us/step\n46 카테고리\n8982 학습용뉴스\n2246 테스트용뉴스\n[1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 2, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 2, 2, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 2, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n\n\n\n# x_train =sequence.pad_sequences(X_train,maxlen=100)\n# x_test =sequence.pad_sequences(X_test,maxlen=100)\n# y_train = np_utils.to_categorical(Y_train) #이전코드\n# y_test = np_utils.to_categorical(Y_test)\n\n\nfrom tensorflow.keras.utils import to_categorical\n\nx_train = sequence.pad_sequences(X_train, maxlen=100)\nx_test = sequence.pad_sequences(X_test, maxlen=100)\ny_train = to_categorical(Y_train)  # np_utils.to_categorical 대신 to_categorical 사용\ny_test = to_categorical(Y_test)\n\n\nmodel = Sequential()\nmodel.add(Embedding(1000,100))\nmodel.add(LSTM(100,activation='tanh'))\nmodel.add(Dense(46,activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\nhistory = model.fit(x_train,y_train,batch_size=100,epochs=20,validation_data=(x_test,y_test))\n\nEpoch 1/20\n90/90 [==============================] - 9s 82ms/step - loss: 2.5827 - accuracy: 0.3460 - val_loss: 2.2901 - val_accuracy: 0.3620\nEpoch 2/20\n90/90 [==============================] - 7s 78ms/step - loss: 2.3067 - accuracy: 0.3917 - val_loss: 2.0693 - val_accuracy: 0.5036\nEpoch 3/20\n90/90 [==============================] - 7s 77ms/step - loss: 2.0138 - accuracy: 0.4958 - val_loss: 1.8937 - val_accuracy: 0.5285\nEpoch 4/20\n90/90 [==============================] - 7s 74ms/step - loss: 1.7928 - accuracy: 0.5442 - val_loss: 1.7842 - val_accuracy: 0.5677\nEpoch 5/20\n90/90 [==============================] - 7s 74ms/step - loss: 1.6979 - accuracy: 0.5669 - val_loss: 1.7077 - val_accuracy: 0.5628\nEpoch 6/20\n90/90 [==============================] - 7s 75ms/step - loss: 1.6262 - accuracy: 0.5826 - val_loss: 1.6480 - val_accuracy: 0.5761\nEpoch 7/20\n90/90 [==============================] - 7s 73ms/step - loss: 1.5504 - accuracy: 0.6097 - val_loss: 1.5901 - val_accuracy: 0.6024\nEpoch 8/20\n90/90 [==============================] - 7s 75ms/step - loss: 1.4472 - accuracy: 0.6346 - val_loss: 1.4807 - val_accuracy: 0.6238\nEpoch 9/20\n90/90 [==============================] - 7s 74ms/step - loss: 1.3671 - accuracy: 0.6525 - val_loss: 1.4441 - val_accuracy: 0.6296\nEpoch 10/20\n90/90 [==============================] - 7s 73ms/step - loss: 1.2649 - accuracy: 0.6828 - val_loss: 1.3604 - val_accuracy: 0.6625\nEpoch 11/20\n90/90 [==============================] - 7s 74ms/step - loss: 1.1974 - accuracy: 0.6939 - val_loss: 1.3254 - val_accuracy: 0.6652\nEpoch 12/20\n90/90 [==============================] - 7s 74ms/step - loss: 1.1243 - accuracy: 0.7096 - val_loss: 1.2726 - val_accuracy: 0.6754\nEpoch 13/20\n90/90 [==============================] - 7s 74ms/step - loss: 1.0576 - accuracy: 0.7306 - val_loss: 1.2636 - val_accuracy: 0.6759\nEpoch 14/20\n90/90 [==============================] - 7s 74ms/step - loss: 1.0125 - accuracy: 0.7390 - val_loss: 1.2701 - val_accuracy: 0.6768\nEpoch 15/20\n90/90 [==============================] - 7s 73ms/step - loss: 0.9626 - accuracy: 0.7554 - val_loss: 1.2027 - val_accuracy: 0.6946\nEpoch 16/20\n90/90 [==============================] - 7s 73ms/step - loss: 0.9166 - accuracy: 0.7675 - val_loss: 1.2308 - val_accuracy: 0.6874\nEpoch 17/20\n90/90 [==============================] - 7s 73ms/step - loss: 0.8661 - accuracy: 0.7816 - val_loss: 1.1993 - val_accuracy: 0.6968\nEpoch 18/20\n90/90 [==============================] - 7s 73ms/step - loss: 0.8367 - accuracy: 0.7877 - val_loss: 1.1866 - val_accuracy: 0.7057\nEpoch 19/20\n90/90 [==============================] - 7s 74ms/step - loss: 0.7863 - accuracy: 0.8044 - val_loss: 1.1989 - val_accuracy: 0.7030\nEpoch 20/20\n90/90 [==============================] - 7s 73ms/step - loss: 0.7522 - accuracy: 0.8096 - val_loss: 1.2209 - val_accuracy: 0.7075\n\n\n\nprint(\"\\n Test Accuracy: %.4f\" % (model.evaluate(x_test,y_test)[1]))\n\ny_vloss = history.history['val_loss']\ny_loss = history.history['loss']\n\nx_len = numpy.arange(len(y_loss))\nplt.plot(x_len,y_vloss,marker='.' , c=\"red\",label='Tesset_loss')\n\nplt.plot(x_len,y_loss,marker='.' , c=\"blue\" , label='Trainset_loss')\n\nplt.legend(loc='upper right')\nplt.grid()\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()\n\n71/71 [==============================] - 1s 11ms/step - loss: 1.2209 - accuracy: 0.7075\n\n Test Accuracy: 0.7075\n\n\n\n\n\n\n\n\n\n\n# 테스트오차가 올라가기직전에 끊어서 earlystopping 을 수동적으로 잘 했다고 볼 수 있다!\n\n\nEmbedding(1000,100) 은 [6] 에서 언급했 듯 1000차원을 100차원으로 축소시킨것\n100차원에서 LSTM 을 이용한 RNN 으로 오차역전파를 사용하며 각각의 노드를 학습\nDense 는 fullyconnected 를 의미하며 풀리커넥티드 46 node 로 layer 층을 softmax 시키기"
  },
  {
    "objectID": "posts/[3]신경망의 이해.html",
    "href": "posts/[3]신경망의 이해.html",
    "title": "[3]신경망의 이해",
    "section": "",
    "text": "신경망의 기본단위\n\n- In deeplearning - $ y = ax + \\(&lt;span style=\"color:red; font-weight:bold;\"&gt;\\)b$\n=&gt; \\(y =\\) \\(w\\)x + \\(b\\)\n 가중치 = \\(w\\)  바이어스 = \\(bias\\)  가중합 = \\(wx\\)  활성화함수 = \\(a\\)\n# 다층 퍼셉트론의 설계\n$ n_1= (x_1w_{{11}} + x_2w{_{21}} + b_1) $\n\n\\(n_1\\) 은 하나의 은닉층내의퍼셉트론의 값입니다.\n\n$ n_2= (x_1w_{{12}} + x_2w{_{22}} + b_1) $\n$y_{out} =(n_1w_{{31}} + x_2w{_{32}} + b_3) $\n\n# W(1) 은 2x2 행렬로 볼수있다.\n\nW_1 = [[w_11 ,w_12] ,[w_21,w_22]]  W_2 = [[w_31],[w_32]]  b(1) = [[3],[1]]  b(2) = [-1]\n\n# 역전파는 생략 -&gt; 그냥 전미분으로 반대쪽으로 가면서 연쇄법칙으로 계산 용이\n\n\n\n #### 역전파가 전달될때마다 기울기의 값이 점점 작아져서 기울기 소실 발생(vanishing gradient)\n\n이유 : sigmoid 의 미분 값이 0.3이 max\n시그모이드, 하이퍼볼릭,렐루,소프트멕스가 이쓴데 렐룰를 쓰면 미분값이 1이되어 맨 처음층이 살아남을 수 있어서 사용됩니다.\n\n- 다른 경사하강법문제 -확률적 경사 하강법 SGD , 모멘텀 , 네스테로프 모멘텀 , 아다그라드 , RMSProp, 아담 등은 다음에 다루겠습니다."
  },
  {
    "objectID": "posts/[3]신경망의 이해.html#기울기-소실-문제",
    "href": "posts/[3]신경망의 이해.html#기울기-소실-문제",
    "title": "[3]신경망의 이해",
    "section": "",
    "text": "#### 역전파가 전달될때마다 기울기의 값이 점점 작아져서 기울기 소실 발생(vanishing gradient)\n\n이유 : sigmoid 의 미분 값이 0.3이 max\n시그모이드, 하이퍼볼릭,렐루,소프트멕스가 이쓴데 렐룰를 쓰면 미분값이 1이되어 맨 처음층이 살아남을 수 있어서 사용됩니다.\n\n- 다른 경사하강법문제 -확률적 경사 하강법 SGD , 모멘텀 , 네스테로프 모멘텀 , 아다그라드 , RMSProp, 아담 등은 다음에 다루겠습니다."
  },
  {
    "objectID": "posts/[1]tensorflow와 기초수학.html",
    "href": "posts/[1]tensorflow와 기초수학.html",
    "title": "[1] tensorflow 를 이용한 딥러닝 모델 실습",
    "section": "",
    "text": "현재는 pytorch 가 상용화된 시점에서 tensorflow 의 장점도 알아볼 수 있는 기회 (tensorflow 는 구글이 만들어서 구글취업에 도움이..)\n\n\n#!pip install tensorflow\n\n\nimport pandas as pd\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\ndf_train\nX = pd.get_dummies(df_train[['toeic','gpa']])\nY = df_train[['employment']]\n\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\n#모델 설정과 딥러닝 구조 결정정\nmodel = Sequential()\nmodel.add(Dense(30, input_dim=2 , activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\n#딥러닝 실행\n\nmodel.compile(loss='mean_squared_error',optimizer='adam',metrics=['accuracy'])\nmodel.fit(X,Y, epochs=100,batch_size=10)\n\n\nEpoch 1/100\n50/50 [==============================] - 0s 784us/step - loss: 0.4801 - accuracy: 0.5020\nEpoch 2/100\n50/50 [==============================] - 0s 758us/step - loss: 0.4109 - accuracy: 0.4860\nEpoch 3/100\n50/50 [==============================] - 0s 677us/step - loss: 0.4088 - accuracy: 0.4960\nEpoch 4/100\n50/50 [==============================] - 0s 654us/step - loss: 0.3409 - accuracy: 0.5060\nEpoch 5/100\n50/50 [==============================] - 0s 659us/step - loss: 0.3178 - accuracy: 0.5080\nEpoch 6/100\n50/50 [==============================] - 0s 642us/step - loss: 0.2569 - accuracy: 0.5620\nEpoch 7/100\n50/50 [==============================] - 0s 640us/step - loss: 0.3526 - accuracy: 0.5360\nEpoch 8/100\n50/50 [==============================] - 0s 636us/step - loss: 0.2452 - accuracy: 0.6060\nEpoch 9/100\n50/50 [==============================] - 0s 644us/step - loss: 0.2451 - accuracy: 0.6400\nEpoch 10/100\n50/50 [==============================] - 0s 649us/step - loss: 0.2393 - accuracy: 0.6400\nEpoch 11/100\n50/50 [==============================] - 0s 646us/step - loss: 0.2290 - accuracy: 0.6160\nEpoch 12/100\n50/50 [==============================] - 0s 647us/step - loss: 0.2045 - accuracy: 0.6900\nEpoch 13/100\n50/50 [==============================] - 0s 641us/step - loss: 0.2870 - accuracy: 0.5860\nEpoch 14/100\n50/50 [==============================] - 0s 648us/step - loss: 0.2420 - accuracy: 0.6280\nEpoch 15/100\n50/50 [==============================] - 0s 650us/step - loss: 0.1949 - accuracy: 0.6980\nEpoch 16/100\n50/50 [==============================] - 0s 646us/step - loss: 0.2214 - accuracy: 0.6720\nEpoch 17/100\n50/50 [==============================] - 0s 660us/step - loss: 0.1891 - accuracy: 0.6980\nEpoch 18/100\n50/50 [==============================] - 0s 644us/step - loss: 0.1848 - accuracy: 0.7160\nEpoch 19/100\n50/50 [==============================] - 0s 650us/step - loss: 0.1732 - accuracy: 0.7480\nEpoch 20/100\n50/50 [==============================] - 0s 645us/step - loss: 0.1818 - accuracy: 0.7500\nEpoch 21/100\n50/50 [==============================] - 0s 639us/step - loss: 0.1649 - accuracy: 0.7720\nEpoch 22/100\n50/50 [==============================] - 0s 667us/step - loss: 0.2311 - accuracy: 0.6560\nEpoch 23/100\n50/50 [==============================] - 0s 3ms/step - loss: 0.2007 - accuracy: 0.6980\nEpoch 24/100\n50/50 [==============================] - 0s 639us/step - loss: 0.1641 - accuracy: 0.7740\nEpoch 25/100\n50/50 [==============================] - 0s 645us/step - loss: 0.1733 - accuracy: 0.7540\nEpoch 26/100\n50/50 [==============================] - 0s 635us/step - loss: 0.2121 - accuracy: 0.6840\nEpoch 27/100\n50/50 [==============================] - 0s 628us/step - loss: 0.1743 - accuracy: 0.7560\nEpoch 28/100\n50/50 [==============================] - 0s 632us/step - loss: 0.1756 - accuracy: 0.7420\nEpoch 29/100\n50/50 [==============================] - 0s 630us/step - loss: 0.1543 - accuracy: 0.7780\nEpoch 30/100\n50/50 [==============================] - 0s 629us/step - loss: 0.1729 - accuracy: 0.7480\nEpoch 31/100\n50/50 [==============================] - 0s 630us/step - loss: 0.1534 - accuracy: 0.7920\nEpoch 32/100\n50/50 [==============================] - 0s 631us/step - loss: 0.1463 - accuracy: 0.7980\nEpoch 33/100\n50/50 [==============================] - 0s 632us/step - loss: 0.1449 - accuracy: 0.8080\nEpoch 34/100\n50/50 [==============================] - 0s 631us/step - loss: 0.1510 - accuracy: 0.7820\nEpoch 35/100\n50/50 [==============================] - 0s 634us/step - loss: 0.1570 - accuracy: 0.7720\nEpoch 36/100\n50/50 [==============================] - 0s 633us/step - loss: 0.1693 - accuracy: 0.7460\nEpoch 37/100\n50/50 [==============================] - 0s 1ms/step - loss: 0.1387 - accuracy: 0.8180\nEpoch 38/100\n50/50 [==============================] - 0s 901us/step - loss: 0.1467 - accuracy: 0.8060\nEpoch 39/100\n50/50 [==============================] - 0s 668us/step - loss: 0.1436 - accuracy: 0.8080\nEpoch 40/100\n50/50 [==============================] - 0s 643us/step - loss: 0.1456 - accuracy: 0.7880\nEpoch 41/100\n50/50 [==============================] - 0s 637us/step - loss: 0.1572 - accuracy: 0.7820\nEpoch 42/100\n50/50 [==============================] - 0s 635us/step - loss: 0.1452 - accuracy: 0.8160\nEpoch 43/100\n50/50 [==============================] - 0s 639us/step - loss: 0.1531 - accuracy: 0.7780\nEpoch 44/100\n50/50 [==============================] - 0s 636us/step - loss: 0.1415 - accuracy: 0.7900\nEpoch 45/100\n50/50 [==============================] - 0s 637us/step - loss: 0.1354 - accuracy: 0.8060\nEpoch 46/100\n50/50 [==============================] - 0s 634us/step - loss: 0.1475 - accuracy: 0.8040\nEpoch 47/100\n50/50 [==============================] - 0s 637us/step - loss: 0.1565 - accuracy: 0.7740\nEpoch 48/100\n50/50 [==============================] - 0s 640us/step - loss: 0.1434 - accuracy: 0.7740\nEpoch 49/100\n50/50 [==============================] - 0s 638us/step - loss: 0.1508 - accuracy: 0.7780\nEpoch 50/100\n50/50 [==============================] - 0s 631us/step - loss: 0.1261 - accuracy: 0.8220\nEpoch 51/100\n50/50 [==============================] - 0s 637us/step - loss: 0.1425 - accuracy: 0.8040\nEpoch 52/100\n50/50 [==============================] - 0s 637us/step - loss: 0.1384 - accuracy: 0.8060\nEpoch 53/100\n50/50 [==============================] - 0s 632us/step - loss: 0.1468 - accuracy: 0.7780\nEpoch 54/100\n50/50 [==============================] - 0s 633us/step - loss: 0.1266 - accuracy: 0.8460\nEpoch 55/100\n50/50 [==============================] - 0s 633us/step - loss: 0.1369 - accuracy: 0.8000\nEpoch 56/100\n50/50 [==============================] - 0s 645us/step - loss: 0.1579 - accuracy: 0.7660\nEpoch 57/100\n50/50 [==============================] - 0s 633us/step - loss: 0.1339 - accuracy: 0.8040\nEpoch 58/100\n50/50 [==============================] - 0s 637us/step - loss: 0.1218 - accuracy: 0.8380\nEpoch 59/100\n50/50 [==============================] - 0s 637us/step - loss: 0.1296 - accuracy: 0.8140\nEpoch 60/100\n50/50 [==============================] - 0s 630us/step - loss: 0.1272 - accuracy: 0.8180\nEpoch 61/100\n50/50 [==============================] - 0s 634us/step - loss: 0.1368 - accuracy: 0.8100\nEpoch 62/100\n50/50 [==============================] - 0s 633us/step - loss: 0.1740 - accuracy: 0.7380\nEpoch 63/100\n50/50 [==============================] - 0s 631us/step - loss: 0.1358 - accuracy: 0.7960\nEpoch 64/100\n50/50 [==============================] - 0s 632us/step - loss: 0.1435 - accuracy: 0.8040\nEpoch 65/100\n50/50 [==============================] - 0s 634us/step - loss: 0.1355 - accuracy: 0.8120\nEpoch 66/100\n50/50 [==============================] - 0s 633us/step - loss: 0.1203 - accuracy: 0.8380\nEpoch 67/100\n50/50 [==============================] - 0s 634us/step - loss: 0.1309 - accuracy: 0.8200\nEpoch 68/100\n50/50 [==============================] - 0s 635us/step - loss: 0.1150 - accuracy: 0.8460\nEpoch 69/100\n50/50 [==============================] - 0s 635us/step - loss: 0.1319 - accuracy: 0.8220\nEpoch 70/100\n50/50 [==============================] - 0s 632us/step - loss: 0.1205 - accuracy: 0.8360\nEpoch 71/100\n50/50 [==============================] - 0s 636us/step - loss: 0.1137 - accuracy: 0.8420\nEpoch 72/100\n50/50 [==============================] - 0s 648us/step - loss: 0.1192 - accuracy: 0.8360\nEpoch 73/100\n50/50 [==============================] - 0s 646us/step - loss: 0.1353 - accuracy: 0.7980\nEpoch 74/100\n50/50 [==============================] - 0s 651us/step - loss: 0.1263 - accuracy: 0.8100\nEpoch 75/100\n50/50 [==============================] - 0s 634us/step - loss: 0.1095 - accuracy: 0.8540\nEpoch 76/100\n50/50 [==============================] - 0s 633us/step - loss: 0.1151 - accuracy: 0.8380\nEpoch 77/100\n50/50 [==============================] - 0s 631us/step - loss: 0.1124 - accuracy: 0.8520\nEpoch 78/100\n50/50 [==============================] - 0s 636us/step - loss: 0.1135 - accuracy: 0.8540\nEpoch 79/100\n50/50 [==============================] - 0s 646us/step - loss: 0.1450 - accuracy: 0.7900\nEpoch 80/100\n50/50 [==============================] - 0s 656us/step - loss: 0.1222 - accuracy: 0.8240\nEpoch 81/100\n50/50 [==============================] - 0s 644us/step - loss: 0.1174 - accuracy: 0.8320\nEpoch 82/100\n50/50 [==============================] - 0s 645us/step - loss: 0.1544 - accuracy: 0.7740\nEpoch 83/100\n50/50 [==============================] - 0s 655us/step - loss: 0.1224 - accuracy: 0.8360\nEpoch 84/100\n50/50 [==============================] - 0s 684us/step - loss: 0.1495 - accuracy: 0.8020\nEpoch 85/100\n50/50 [==============================] - 0s 644us/step - loss: 0.1197 - accuracy: 0.8400\nEpoch 86/100\n50/50 [==============================] - 0s 655us/step - loss: 0.1442 - accuracy: 0.7800\nEpoch 87/100\n50/50 [==============================] - 0s 644us/step - loss: 0.1242 - accuracy: 0.8240\nEpoch 88/100\n50/50 [==============================] - 0s 640us/step - loss: 0.1278 - accuracy: 0.8220\nEpoch 89/100\n50/50 [==============================] - 0s 631us/step - loss: 0.1658 - accuracy: 0.7580\nEpoch 90/100\n50/50 [==============================] - 0s 632us/step - loss: 0.1258 - accuracy: 0.8280\nEpoch 91/100\n50/50 [==============================] - 0s 631us/step - loss: 0.1195 - accuracy: 0.8340\nEpoch 92/100\n50/50 [==============================] - 0s 632us/step - loss: 0.1360 - accuracy: 0.8040\nEpoch 93/100\n50/50 [==============================] - 0s 633us/step - loss: 0.1151 - accuracy: 0.8340\nEpoch 94/100\n50/50 [==============================] - 0s 632us/step - loss: 0.1277 - accuracy: 0.8100\nEpoch 95/100\n50/50 [==============================] - 0s 638us/step - loss: 0.1264 - accuracy: 0.8240\nEpoch 96/100\n50/50 [==============================] - 0s 673us/step - loss: 0.1080 - accuracy: 0.8600\nEpoch 97/100\n50/50 [==============================] - 0s 632us/step - loss: 0.1020 - accuracy: 0.8560\nEpoch 98/100\n50/50 [==============================] - 0s 631us/step - loss: 0.1100 - accuracy: 0.8440\nEpoch 99/100\n50/50 [==============================] - 0s 629us/step - loss: 0.1109 - accuracy: 0.8440\nEpoch 100/100\n50/50 [==============================] - 0s 634us/step - loss: 0.1040 - accuracy: 0.8580\n\n\n&lt;keras.src.callbacks.History at 0x7f65a416cb50&gt;\n\n\n\nyhat = model.predict(X)\n\n16/16 [==============================] - 0s 651us/step\n\n\n\ny, yhat\n\n(     employment\n 0             0\n 1             0\n 2             0\n 3             0\n 4             1\n ..          ...\n 495           1\n 496           1\n 497           0\n 498           0\n 499           1\n \n [500 rows x 1 columns],\n array([[0.01917117],\n        [0.27701277],\n        [0.57248056],\n        [0.07291598],\n        [0.9337903 ],\n        [0.16967887],\n        [0.04423696],\n        [0.16271544],\n        [0.82395774],\n        [0.03679871],\n        [0.49401855],\n        [0.70155656],\n        [0.74422747],\n        [0.07437798],\n        [0.9701731 ],\n        [0.11984617],\n        [0.22355464],\n        [0.9713866 ],\n        [0.84243065],\n        [0.34956658],\n        [0.93221444],\n        [0.79003614],\n        [0.8277889 ],\n        [0.78845423],\n        [0.67572665],\n        [0.91666716],\n        [0.900001  ],\n        [0.4948617 ],\n        [0.08570924],\n        [0.08675035],\n        [0.45085144],\n        [0.09457831],\n        [0.89941543],\n        [0.89091516],\n        [0.98111194],\n        [0.38627425],\n        [0.500065  ],\n        [0.07303426],\n        [0.2076528 ],\n        [0.39872044],\n        [0.14696163],\n        [0.10400353],\n        [0.849859  ],\n        [0.6330585 ],\n        [0.50127107],\n        [0.12333758],\n        [0.8664031 ],\n        [0.27620575],\n        [0.8914864 ],\n        [0.25114718],\n        [0.8564444 ],\n        [0.86841565],\n        [0.90393656],\n        [0.57267356],\n        [0.07932395],\n        [0.06170203],\n        [0.18767741],\n        [0.24924625],\n        [0.95436203],\n        [0.07904566],\n        [0.03398843],\n        [0.38441926],\n        [0.78319013],\n        [0.8257212 ],\n        [0.85713553],\n        [0.3929678 ],\n        [0.12670748],\n        [0.95258856],\n        [0.06235125],\n        [0.310451  ],\n        [0.36083743],\n        [0.32625353],\n        [0.9480908 ],\n        [0.66629976],\n        [0.27613765],\n        [0.24882582],\n        [0.5382514 ],\n        [0.9798404 ],\n        [0.9588324 ],\n        [0.9194008 ],\n        [0.63055253],\n        [0.5876403 ],\n        [0.2970832 ],\n        [0.8484085 ],\n        [0.89806634],\n        [0.40804952],\n        [0.9729422 ],\n        [0.87672734],\n        [0.933373  ],\n        [0.73566663],\n        [0.98569727],\n        [0.7418684 ],\n        [0.90864396],\n        [0.8763635 ],\n        [0.7899342 ],\n        [0.7961238 ],\n        [0.10964687],\n        [0.91110736],\n        [0.05452807],\n        [0.42326584],\n        [0.8430349 ],\n        [0.8250593 ],\n        [0.57293916],\n        [0.17664742],\n        [0.03240925],\n        [0.40322167],\n        [0.7187368 ],\n        [0.40607226],\n        [0.88016856],\n        [0.48957664],\n        [0.98425466],\n        [0.29335815],\n        [0.93182445],\n        [0.09154762],\n        [0.91493297],\n        [0.8811433 ],\n        [0.64865637],\n        [0.03991823],\n        [0.94465333],\n        [0.8167183 ],\n        [0.01954143],\n        [0.05376468],\n        [0.57212585],\n        [0.05252383],\n        [0.9681868 ],\n        [0.96812886],\n        [0.94278234],\n        [0.9837329 ],\n        [0.5691354 ],\n        [0.88369924],\n        [0.11227809],\n        [0.53355527],\n        [0.87268454],\n        [0.35719532],\n        [0.27300215],\n        [0.67220396],\n        [0.97631776],\n        [0.8732753 ],\n        [0.9843648 ],\n        [0.9656994 ],\n        [0.95308733],\n        [0.6931385 ],\n        [0.81876254],\n        [0.96818775],\n        [0.5238588 ],\n        [0.8196187 ],\n        [0.8168527 ],\n        [0.58253735],\n        [0.88458246],\n        [0.83249867],\n        [0.1201875 ],\n        [0.8826075 ],\n        [0.93429315],\n        [0.77423817],\n        [0.02924814],\n        [0.38178807],\n        [0.3466166 ],\n        [0.9653925 ],\n        [0.79927737],\n        [0.22591524],\n        [0.07185473],\n        [0.82701117],\n        [0.05066053],\n        [0.23043263],\n        [0.41072193],\n        [0.92123693],\n        [0.90527797],\n        [0.76721424],\n        [0.6522743 ],\n        [0.34334943],\n        [0.63762605],\n        [0.19859885],\n        [0.97158444],\n        [0.8913412 ],\n        [0.08545599],\n        [0.95702195],\n        [0.9546011 ],\n        [0.7769809 ],\n        [0.6955107 ],\n        [0.959477  ],\n        [0.13949394],\n        [0.88012   ],\n        [0.9681104 ],\n        [0.16866204],\n        [0.81970745],\n        [0.06679367],\n        [0.50428295],\n        [0.97363365],\n        [0.13436674],\n        [0.73526573],\n        [0.3802947 ],\n        [0.3797849 ],\n        [0.92563415],\n        [0.93689096],\n        [0.17957757],\n        [0.14559893],\n        [0.8678163 ],\n        [0.88208693],\n        [0.97858894],\n        [0.6803263 ],\n        [0.88837254],\n        [0.6473557 ],\n        [0.713776  ],\n        [0.98046607],\n        [0.40170547],\n        [0.9541457 ],\n        [0.33725163],\n        [0.07588536],\n        [0.19491214],\n        [0.61104286],\n        [0.14010936],\n        [0.91630405],\n        [0.03834961],\n        [0.72811943],\n        [0.34053385],\n        [0.62020403],\n        [0.07496268],\n        [0.3621057 ],\n        [0.77371186],\n        [0.79291373],\n        [0.24495268],\n        [0.14030856],\n        [0.98848253],\n        [0.21629004],\n        [0.77385217],\n        [0.4226013 ],\n        [0.5461467 ],\n        [0.06345365],\n        [0.9591146 ],\n        [0.9653101 ],\n        [0.9109111 ],\n        [0.59520805],\n        [0.12937342],\n        [0.94690955],\n        [0.31774738],\n        [0.07824167],\n        [0.4035803 ],\n        [0.970306  ],\n        [0.86459106],\n        [0.83906114],\n        [0.8583879 ],\n        [0.45989472],\n        [0.9608893 ],\n        [0.6475749 ],\n        [0.9643738 ],\n        [0.5300669 ],\n        [0.40387452],\n        [0.03868933],\n        [0.4975298 ],\n        [0.4583777 ],\n        [0.05485775],\n        [0.8855906 ],\n        [0.29030403],\n        [0.92123866],\n        [0.41690975],\n        [0.97260654],\n        [0.98183936],\n        [0.05801743],\n        [0.9789199 ],\n        [0.94818807],\n        [0.3699752 ],\n        [0.3638382 ],\n        [0.5479293 ],\n        [0.47763076],\n        [0.51313263],\n        [0.49031878],\n        [0.95143396],\n        [0.8830845 ],\n        [0.04611275],\n        [0.8867892 ],\n        [0.76889664],\n        [0.85455334],\n        [0.18544999],\n        [0.4523961 ],\n        [0.6139288 ],\n        [0.8853227 ],\n        [0.685312  ],\n        [0.01474617],\n        [0.7034919 ],\n        [0.79860747],\n        [0.57097834],\n        [0.72747326],\n        [0.08300006],\n        [0.9866009 ],\n        [0.9491896 ],\n        [0.4675123 ],\n        [0.49861518],\n        [0.8898536 ],\n        [0.67578405],\n        [0.8755042 ],\n        [0.28693882],\n        [0.7925204 ],\n        [0.32779872],\n        [0.99298096],\n        [0.15786973],\n        [0.05273386],\n        [0.07585184],\n        [0.84367484],\n        [0.11621627],\n        [0.3452456 ],\n        [0.08934698],\n        [0.4832032 ],\n        [0.5860667 ],\n        [0.6686725 ],\n        [0.05702977],\n        [0.21380268],\n        [0.8648583 ],\n        [0.7823888 ],\n        [0.24898165],\n        [0.28666323],\n        [0.20016862],\n        [0.2168708 ],\n        [0.21898386],\n        [0.4306856 ],\n        [0.82605165],\n        [0.36329412],\n        [0.94152397],\n        [0.97779167],\n        [0.7131194 ],\n        [0.81928974],\n        [0.03559082],\n        [0.22447224],\n        [0.63786954],\n        [0.95887583],\n        [0.22680953],\n        [0.03871967],\n        [0.22801438],\n        [0.9555246 ],\n        [0.99244696],\n        [0.42094892],\n        [0.33412814],\n        [0.97125894],\n        [0.84219325],\n        [0.9215936 ],\n        [0.11309428],\n        [0.9716536 ],\n        [0.32081524],\n        [0.6802617 ],\n        [0.33729896],\n        [0.02772723],\n        [0.8084843 ],\n        [0.6887507 ],\n        [0.841458  ],\n        [0.9155224 ],\n        [0.09881844],\n        [0.73725986],\n        [0.30792457],\n        [0.5606182 ],\n        [0.05556777],\n        [0.8924346 ],\n        [0.8027948 ],\n        [0.6102546 ],\n        [0.02506168],\n        [0.9593257 ],\n        [0.32803148],\n        [0.05519449],\n        [0.1393818 ],\n        [0.85458094],\n        [0.6028396 ],\n        [0.10856601],\n        [0.8255928 ],\n        [0.13584475],\n        [0.50551504],\n        [0.96753633],\n        [0.9437374 ],\n        [0.8593406 ],\n        [0.282129  ],\n        [0.93644136],\n        [0.60541344],\n        [0.908679  ],\n        [0.76774406],\n        [0.89691186],\n        [0.2703746 ],\n        [0.64322895],\n        [0.29612315],\n        [0.04659881],\n        [0.20838122],\n        [0.49536982],\n        [0.9574271 ],\n        [0.9164298 ],\n        [0.6172774 ],\n        [0.2583753 ],\n        [0.51643944],\n        [0.4519163 ],\n        [0.75248504],\n        [0.94678587],\n        [0.18314745],\n        [0.02212726],\n        [0.05176777],\n        [0.97237957],\n        [0.5470983 ],\n        [0.44097656],\n        [0.86453414],\n        [0.29728734],\n        [0.92869407],\n        [0.73852324],\n        [0.22985455],\n        [0.9173276 ],\n        [0.64710844],\n        [0.421796  ],\n        [0.4742776 ],\n        [0.5329712 ],\n        [0.6139735 ],\n        [0.14020257],\n        [0.8466091 ],\n        [0.10136232],\n        [0.9823159 ],\n        [0.78644043],\n        [0.24029979],\n        [0.44837937],\n        [0.95112103],\n        [0.98899335],\n        [0.72358245],\n        [0.9876894 ],\n        [0.13090147],\n        [0.80479366],\n        [0.18592311],\n        [0.10133642],\n        [0.18840505],\n        [0.05062044],\n        [0.9519586 ],\n        [0.8140401 ],\n        [0.04401746],\n        [0.26882783],\n        [0.95578164],\n        [0.30525184],\n        [0.06306392],\n        [0.78950936],\n        [0.0482501 ],\n        [0.48453936],\n        [0.48090762],\n        [0.66874796],\n        [0.9488091 ],\n        [0.86852896],\n        [0.4179146 ],\n        [0.9688493 ],\n        [0.02599564],\n        [0.98675174],\n        [0.41479954],\n        [0.5969038 ],\n        [0.74173814],\n        [0.91931564],\n        [0.01778276],\n        [0.9395381 ],\n        [0.9695743 ],\n        [0.7976663 ],\n        [0.9694414 ],\n        [0.9728497 ],\n        [0.5978612 ],\n        [0.88861656],\n        [0.6589707 ],\n        [0.6629037 ],\n        [0.24528511],\n        [0.5953964 ],\n        [0.9042165 ],\n        [0.9857699 ],\n        [0.30555132],\n        [0.94010955],\n        [0.18784209],\n        [0.2734823 ],\n        [0.50732434],\n        [0.936694  ],\n        [0.41610953],\n        [0.7398132 ],\n        [0.57886773],\n        [0.8904059 ],\n        [0.76505774],\n        [0.27815723],\n        [0.938038  ],\n        [0.16718833],\n        [0.85088605],\n        [0.20372394],\n        [0.8189768 ],\n        [0.12603728],\n        [0.62040645],\n        [0.47599888],\n        [0.41412124],\n        [0.96225524],\n        [0.32949308],\n        [0.77854615],\n        [0.50865686],\n        [0.98446643],\n        [0.6660308 ],\n        [0.30000117],\n        [0.31887043],\n        [0.34211856],\n        [0.66088057],\n        [0.98051924],\n        [0.38630512],\n        [0.31678852],\n        [0.22271015],\n        [0.3595858 ],\n        [0.04029208],\n        [0.29222274],\n        [0.7604949 ],\n        [0.9275784 ],\n        [0.56702435],\n        [0.02502951],\n        [0.12665989],\n        [0.8747921 ]], dtype=float32))"
  },
  {
    "objectID": "posts/[1]tensorflow와 기초수학.html#선형회귀",
    "href": "posts/[1]tensorflow와 기초수학.html#선형회귀",
    "title": "[1] tensorflow 를 이용한 딥러닝 모델 실습",
    "section": "선형회귀",
    "text": "선형회귀\n\n회귀분석에서 기울기와 절편 등 중회귀모형에서 직접적으로 구하는 방법을 구한다.\n\n식을 구하는 방식을 간단히 단순선형모형에서 식으로 구하기\n식을 행렬의 표현을 이용하여 중회귀모형에서 직접 쉽게 구하기.(행렬은 증명을 쉽게 해준다는 장점이 존재)"
  },
  {
    "objectID": "posts/[1]tensorflow와 기초수학.html#최소제곱법으로-그은-선에-대한-평가가-필요하다.",
    "href": "posts/[1]tensorflow와 기초수학.html#최소제곱법으로-그은-선에-대한-평가가-필요하다.",
    "title": "[1] tensorflow 를 이용한 딥러닝 모델 실습",
    "section": "최소제곱법으로 그은 선에 대한 평가가 필요하다.",
    "text": "최소제곱법으로 그은 선에 대한 평가가 필요하다.\n\n최소제곱법은 기울기와 절편이 최소이지만 이 모형이 제대로 우리의 데이터를 표현할 수 없다.\n\n그 이유는 우리의 데이터가 전체의 데이터가 아니라 표본이기 때문이다. 그리고 오차가 존재하기 때문이다. 등 더 많은 문제가 있다. - 선에대한 평가의 지표를 만들자\n\n평균제곱오차 MSE 이것이 그의 지표로써 이용된다.\nMSE = $ _{i=1}^{n} (y_i - _i)^2 $ 인데 단순회귀모형에서는 $ n -&gt; n-2$ , 원점을 지나는 단순회귀모형은$ n -&gt; n-1 $이런식으로 자유도에 따라서 식이 바뀐다.\n\n\n- 평가방법은 수리통계학에서의 작은 MSE 찾는 방법과 회귀분석에서의 MSE 계산으로 각종 지표로 유용한 직선인지 평가할 수 있다."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "newblog",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nDec 31, 2023\n\n\n[9]GAN 실행과 마무리정리 (필독)\n\n\n전준한 \n\n\n\n\nDec 26, 2023\n\n\n[8]GAN\n\n\n전준한 \n\n\n\n\nDec 24, 2023\n\n\n[7]RNN\n\n\n전준한 \n\n\n\n\nDec 23, 2023\n\n\n[6]자연어처리연습\n\n\n전준한 \n\n\n\n\nDec 23, 2023\n\n\n[3]신경망의 이해\n\n\n전준한 \n\n\n\n\nDec 23, 2023\n\n\n[4]딥러닝기초실습\n\n\n전준한 \n\n\n\n\nDec 22, 2023\n\n\n[2]경사하강법과 다중선형회귀\n\n\n전준한 \n\n\n\n\nDec 22, 2023\n\n\n[1] tensorflow 를 이용한 딥러닝 모델 실습\n\n\n전준한 \n\n\n\n\n\nNo matching items"
  }
]